{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bd0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from importlib import reload\n",
    "here = os.getcwd()\n",
    "sys.path.append(os.path.join(here,\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a82e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17087706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cde.cde_data_common import process_data,get_final_linear_input_channels,get_final_indices,wrap_data,augment_data\n",
    "import models.cde.cde_train_common as train_common\n",
    "import models.cde.cde_data_common as cde_data_common\n",
    "import models.cde as cde\n",
    "\n",
    "from utils.test_utils import make_results_filenames\n",
    "\n",
    "import explainer.rule_pattern_miner as rlm\n",
    "import explainer.explainer_utils as eutils\n",
    "from explainer.FPGrowth_tree import *\n",
    "import explainer.itemsets_miner as itm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e560be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,confusion_matrix,precision_score,recall_score,accuracy_score,roc_auc_score,roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033ca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree,export_text,export_graphviz\n",
    "import explainer.DT_rules as dtr\n",
    "from explainer.DT_rules import obtain_rule_lists_from_DT,select_rule_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc10ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "def download(save_path):\n",
    "\n",
    "    loc_Azip = os.path.join(save_path, 'training_setA.zip')\n",
    "    loc_Bzip = os.path.join(save_path, 'training_setB.zip')\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "    if not os.path.exists(loc_Azip):\n",
    "        \n",
    "        urllib.request.urlretrieve('https://archive.physionet.org/users/shared/challenge-2019/training_setA.zip',\n",
    "                                   str(loc_Azip))\n",
    "        urllib.request.urlretrieve('https://archive.physionet.org/users/shared/challenge-2019/training_setB.zip',\n",
    "                                   str(loc_Bzip))\n",
    "\n",
    "        with zipfile.ZipFile(loc_Azip, 'r') as f:\n",
    "            f.extractall(str(save_path))\n",
    "        with zipfile.ZipFile(loc_Bzip, 'r') as f:\n",
    "            f.extractall(str(save_path))\n",
    "        for folder in ('training', 'training_setB'):\n",
    "            for filename in os.listdir(os.path.join(save_path,folder)):\n",
    "                if os.path.exists(os.path.join(save_path,filename)):\n",
    "                    raise RuntimeError\n",
    "                os.rename(os.path.join(save_path, folder, filename), os.path.join(save_path, filename))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75e94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_make_model():\n",
    "    model, regularise = make_model()\n",
    "    model.linear.weight.register_hook(lambda grad: 100 * grad)\n",
    "    model.linear.bias.register_hook(lambda grad: 100 * grad)\n",
    "    return model, regularise\n",
    "\n",
    "def group_processed_data(X,y,times):\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    final_indices,_ = get_final_indices(times,y)\n",
    "    coeffs = process_data(times,X,intensity=intensity,time_intensity=time_intensity,cummean=cummean,cumsum=cumsum,append_times=append_times,interpolate=interpolate)\n",
    "    return coeffs,y,final_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdba56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Control randomness for reproducibility\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2a2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ncde\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa8b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration of NCDE model\n",
    "\n",
    "interpolate = \"linear\"    \n",
    "side_input = False\n",
    "concat_z = True\n",
    "\n",
    "append_times = False\n",
    "time_intensity = True\n",
    "intensity = True \n",
    "static_intensity = True\n",
    "\n",
    "\n",
    "time_len = 72\n",
    "max_epochs = 100\n",
    "pos_weight = 20\n",
    "\n",
    "hidden_channels = 2 \n",
    "hidden_hidden_channels = 128\n",
    "num_hidden_layers = 4\n",
    "\n",
    "batch_size = 1024\n",
    "max_epochs = 500\n",
    "lr = 0.0001 * (batch_size / 64)\n",
    "\n",
    "num_classes = 2\n",
    "cummean = True\n",
    "cumsum = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73499cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify data path and results path\n",
    "dpath = \"../../data/\"\n",
    "if not os.path.exists(dpath):\n",
    "    os.mkdir(dpath)\n",
    "\n",
    "data_folder = os.path.join(dpath,\"sepsis\")\n",
    "\n",
    "rpath = \"./results/sepsis\"\n",
    "if not os.path.exists(rpath):\n",
    "    os.mkdir(rpath)\n",
    "    \n",
    "name = model_name + \"_\" + str(seed)\n",
    "\n",
    "model_path = os.path.join(rpath,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a2a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download data if not downloaded yet ###\n",
    "if not os.path.exists(data_folder):     \n",
    "    download(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31eb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfd81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read raw data\n",
    "\n",
    "X_times = []\n",
    "X_static = []\n",
    "y = []\n",
    "H = time_len\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith('.psv'):\n",
    "        with open(os.path.join(data_folder,filename)) as file:\n",
    "            time = []\n",
    "            label = 0.0\n",
    "            reader = csv.reader(file, delimiter='|')\n",
    "            reader = iter(reader)\n",
    "            next(reader)  # first line is headings\n",
    "            prev_iculos = 0\n",
    "            for line in reader:\n",
    "                assert len(line) == 41\n",
    "                # time values are 34 features\n",
    "                *time_values, age, gender, unit1, unit2, hospadmtime, iculos, sepsislabel = line\n",
    "                iculos = int(iculos)\n",
    "                #print('iculos',iculos)\n",
    "                if iculos > H:  # keep at most the first H hours\n",
    "                    break\n",
    "                ## padding nan for missing hours\n",
    "                for iculos_ in range(prev_iculos + 1, iculos):\n",
    "                    time.append([float('nan') for value in time_values])\n",
    "                    #time.append(np.zeros(len(time_values))+np.nan)\n",
    "                prev_iculos = iculos\n",
    "                time.append([float(value) for value in time_values])\n",
    "                label = max(label, float(sepsislabel))\n",
    "            unit1 = float(unit1)\n",
    "            unit2 = float(unit2)\n",
    "            unit1_obs = not math.isnan(unit1)\n",
    "            unit2_obs = not math.isnan(unit2)\n",
    "            if not unit1_obs:\n",
    "                unit1 = 0.\n",
    "            if not unit2_obs:\n",
    "                unit2 = 0.\n",
    "            hospadmtime = float(hospadmtime)\n",
    "            if math.isnan(hospadmtime):\n",
    "                hospadmtime = 0.  # this only happens for one record\n",
    "            static = [float(age), float(gender), unit1, unit2, hospadmtime]\n",
    "            if static_intensity:\n",
    "                static += [unit1_obs, unit2_obs]\n",
    "            if len(time) > 2:\n",
    "                if len(time) < H:\n",
    "                    # padding less hours\n",
    "                    for t in range(H-len(time)):\n",
    "                        time.append([float('nan') for value in time_values])\n",
    "                X_times.append(time)\n",
    "                X_static.append(static)\n",
    "                y.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7accf201",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess data\n",
    "\n",
    "times = np.arange(time_len).astype(np.float32)\n",
    "times = torch.tensor(times)\n",
    "\n",
    "X_times = np.array(X_times).astype(np.float32)\n",
    "X_static = np.array(X_static).astype(np.float32)\n",
    "y = np.array(y).astype(np.float32)\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_times,y,test_size=0.2,random_state=seed)\n",
    "X_test,X_val,y_test,y_val = train_test_split(X_test,y_test,test_size=0.5,random_state=seed)\n",
    "\n",
    "### min-max normalization\n",
    "for c in range(X_times.shape[-1]):\n",
    "    mi = X_train[:,:,c][~np.isnan(X_train[:,:,c])].min()\n",
    "    ma = X_train[:,:,c][~np.isnan(X_train[:,:,c])].max()\n",
    "    X_train[:,:,c] = (X_train[:,:,c] - mi)/(ma - mi)+1.\n",
    "    X_test[:,:,c] = (X_test[:,:,c] - mi)/(ma - mi)+1.\n",
    "    X_val[:,:,c] = (X_val[:,:,c] - mi)/(ma - mi)+1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ba0bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check X torch.Size([32266, 72, 136])\n",
      "check X torch.Size([4033, 72, 136])\n",
      "check X torch.Size([4034, 72, 136])\n"
     ]
    }
   ],
   "source": [
    "## augment data with cumulative intensities and linear interploation\n",
    "\n",
    "X_train_raw = augment_data(torch.tensor(X_train),times,intensity=intensity,time_intensity=time_intensity,cummean=cummean,cumsum=cumsum,append_times=append_times)\n",
    "X_test_raw = augment_data(torch.tensor(X_test),times,intensity=intensity,time_intensity=time_intensity,cummean=cummean,cumsum=cumsum,append_times=append_times)\n",
    "X_val_raw = augment_data(torch.tensor(X_val),times,intensity=intensity,time_intensity=time_intensity,cummean=cummean,cumsum=cumsum,append_times=append_times)\n",
    "\n",
    "train_data = group_processed_data(X_train,y_train,times)\n",
    "test_data = group_processed_data(X_test,y_test,times)\n",
    "val_data = group_processed_data(X_val,y_val,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e28b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the NCDE model\n",
    "\n",
    "input_channels = train_data[0][0].shape[-1]\n",
    "output_channels = 1\n",
    "stream = True if concat_z else False\n",
    "\n",
    "if concat_z or side_input:\n",
    "    side_input_dim = cv_sets[0][0][-1].shape[-1] if side_input else 0\n",
    "    final_linear_input_channels = get_final_linear_input_channels(hidden_channels,side_input_dim=side_input_dim,time_len=time_len)\n",
    "else:\n",
    "    final_linear_input_channels = None\n",
    "\n",
    "make_model = train_common.make_model(model_name, input_channels, output_channels, hidden_channels,\n",
    "                               hidden_hidden_channels, num_hidden_layers, use_intensity=False,\n",
    "                              final_linear_input_channels=final_linear_input_channels, \n",
    "                              initial=True,side_input=side_input,append_times=append_times,interpolate=interpolate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f6cbd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolate linear\n"
     ]
    }
   ],
   "source": [
    "## train the model if no trained model available \n",
    "if not os.path.exists(model_path):\n",
    "    times, train_dataloader, val_dataloader, test_dataloader = wrap_data(times, train_data, val_data, test_data, device,\n",
    "                                                                                        batch_size=batch_size,num_workers=0)\n",
    "\n",
    "    model,log,log_num = train_common.main(name, times, train_dataloader, val_dataloader, test_dataloader, device,\n",
    "                       new_make_model, num_classes, max_epochs, lr, kwargs={'stream':stream}, pos_weight=torch.tensor(pos_weight),\n",
    "                       step_mode=True,rpath=rpath)\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "else:\n",
    "    # load model if already trained and saved it\n",
    "    log_num = 0 #change to the model number that need to be loaded\n",
    "    model_loc = os.path.join(model_path,\"model_\"+str(log_num)) \n",
    "    if os.path.exists(model_loc):\n",
    "        model, regularise_parameters = make_model()\n",
    "        model.load_state_dict(torch.load(model_loc))\n",
    "        model.to('cpu')\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d106f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get feature names of augmented features\n",
    "\n",
    "raw_feature_names = ['HR','O2Sat','Temp','SBP','MAP','DBP','Resp','EtCO2','BaseExcess','HCO3','FiO2','pH','PaCO2','SaO2',\n",
    "                    'AST','BUN','Alkalinephos','Calcium','Chloride','Creatinine','Bilirubin_direct','Glucose','Lactate','Magnesium',\n",
    "                    'Phosphate','Potassium','Bilirubin_total','TroponinI','Hct','Hgb','PTT','WBC','Fibrinogen','Platelets']\n",
    "\n",
    "latent_feature_names = [r'$z_'+str(i)+'(t_{'+str(h)+'})$' for h in range(time_len) for i in range(hidden_channels) ]\n",
    "intensity_feature_names = [rf+'_ctime' for rf in raw_feature_names] + [rf+'_cmax' for rf in raw_feature_names] + [rf+'_cmean' for rf in raw_feature_names] \n",
    "input_feature_names = raw_feature_names + intensity_feature_names\n",
    "feature_types = [\"int\" if \"ctime\" in fn else \"float\" for fn in input_feature_names] \n",
    "len(input_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce44cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get baseline and test samples for computing impact score matrix\n",
    "\n",
    "model.append_times=append_times\n",
    "train_reps = model.latent_representation(X_train_raw,times=times).detach()\n",
    "test_reps = model.latent_representation(X_test_raw,times=times).detach()\n",
    "val_reps = model.latent_representation(X_val_raw,times=times).detach()\n",
    "\n",
    "baselines = eutils.gen_intgrad_baselines(X_train_raw,y_train,train_reps)\n",
    "subset = eutils.gen_balanced_subset(X_train_raw,y_train,size_per_class=int(y_train.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e300cda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get impact score matrix of latent states\n",
    "int_g, z_shift = [], []\n",
    "for c in range(2):\n",
    "    int_g_c, z_shift_c = eutils.calc_baselines_intg(test_examples=subset,model=model,baselines=baselines,times=times,target_c=c,target_dim=0,C=2)\n",
    "    int_g.append(int_g_c)\n",
    "    z_shift.append(z_shift_c)\n",
    "\n",
    "int_g = torch.vstack(int_g)\n",
    "z_shift = torch.vstack(z_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be532b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get predicted probability of training, test, and validation sets\n",
    "\n",
    "linear_prams = []\n",
    "for p in model.linear.parameters():\n",
    "    p = p.detach()\n",
    "    linear_prams.append(p)\n",
    "\n",
    "pred_y_train = model.linear(train_reps.reshape(train_reps.shape[0],-1)).detach().numpy()\n",
    "pred_y_test = model.linear(test_reps.reshape(test_reps.shape[0],-1)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7287752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y threshold 0.5414686\n"
     ]
    }
   ],
   "source": [
    "## get prediction threshold by roc\n",
    "\n",
    "pred_y = sp.special.expit(pred_y_train).reshape(-1)\n",
    "auc = roc_auc_score(y_train, pred_y)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, pred_y)\n",
    "y_thd = thresholds[np.argmax(tpr - fpr)]\n",
    "print(\"y threshold\",y_thd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c0ed82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441754285578046"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## AUC on test set\n",
    "auc = roc_auc_score(y_test, pred_y_test)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87372fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aadf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If test the rule extraction methods for multiple random seeds, change the random seed here.\n",
    "# # If change random seed in the begining, the model, the training, and test set will change as well, the randomness will have several sources.\n",
    "\n",
    "# seed = 0\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedcc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get impact score matrix for output\n",
    "\n",
    "C=2  ## two classes\n",
    "baseline_reps = model.latent_representation(baselines,times=times)\n",
    "baseline_output = model.linear(baseline_reps.reshape(baseline_reps.shape[0],-1)).detach()\n",
    "\n",
    "subset_reps,subset_output = [],[]\n",
    "for k in range(C):\n",
    "    subset_reps.append(model.latent_representation(subset[k],times=times))\n",
    "    subset_output.append(model.linear(subset_reps[k].reshape(subset_reps[k].shape[0],-1)).detach())\n",
    "    \n",
    "cids = np.arange(C)\n",
    "yshift = []\n",
    "for k in cids:\n",
    "    for kk in cids[cids!=k]:\n",
    "        yshift.append(subset_output[k]-baseline_output[kk])\n",
    "yshift = torch.vstack(yshift)\n",
    "\n",
    "weights = linear_prams[0].reshape(-1,hidden_channels)\n",
    "y_int_g = eutils.output_intg_score(int_g,weights,yshift)\n",
    "y_int_g[torch.isnan(y_int_g)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74980f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04055276381909548"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get impact score threshold \n",
    "thds = np.linspace(0.01,0.2,200)\n",
    "f_n = []\n",
    "for thd in thds:\n",
    "    mask = torch.abs(y_int_g) >= thd \n",
    "    f_n.append((thd,(mask.sum(dim=0)>=len(y_int_g)*0.99).sum()))\n",
    "    if f_n[-1][1]==1:\n",
    "        break\n",
    "thd = f_n[-1][0]\n",
    "thd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fe1d145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature set [8741 9693 6701 9285 7109 9557 7789 9149 8059 6565 8333 7515 6971 6291\n",
      " 6837 7243 6970 8467 8058 7242 7653 9011 6290 8063 7381 8466 7519 7514\n",
      " 7379 5746 7378 6157 8471 9010 5747 7651 6021 6973 6295 7925 8197 9696\n",
      " 8330 8742 6698 7247 9015 6975 8331 5477 7786 5882 8336 9690 6699 7655\n",
      " 7792 5749 8875 9694 5883 7110 8739 7654 5613 7650 8743 9286 5885 7383\n",
      " 8195 7787 7112 9419 9418 5067 8061 8064 8874 5205 9150 5751 8199 8605\n",
      " 4389 9560 8198 8879 4935 5071 6704 7790 4931 7384 6976 9558 5066 6427\n",
      " 8194 6838 6562 9691 4525 5887 7248 8603 9423 7518 8335 6426 8470 7245\n",
      " 6294 5474 7922 7106 8738 9692 5752 4930 6568 9554 8740 6703 8607 6563\n",
      " 9287 5475 8472 8744 9288 4797 6431 8602 8334 7928 3981 9283 6839 4799\n",
      " 7520 3573 8606 9016 9014 3165 9151 5888 9284 9152 9421 7923 4795 9147\n",
      " 9424 9556 8878 6835 5069 7108 7791 9148 6296 4253 2893 6566 5480 9700\n",
      " 4119 7116 5479 6160 8877 6158 7788 7652 4115 3029 9555 6022 5072 5207\n",
      " 7246 8332 6840 7796 8196 6702 3845 8062 5341 6836 4794 5206 7926 8748\n",
      " 4114 8340 9564 6154 5614 6708 5203 3843 6024 4934 9292 4798 3847 4661\n",
      " 6572 6564 6700 3842 6844 3709 7517 8468 9156 8604 7516 6432 6156 2621\n",
      " 6020 4390 6164 5616 6292 7924 6429 7660 9695 6567 5756 5484 3172 7388\n",
      " 7107 5612 7927 5204 6028 2900 6293 4250 3437 4117 4396 4936 4522 8880\n",
      " 8876 9012 4528 4388 3982 3580 2213 4251 7932 6980 5476 8204 2623 5620\n",
      " 4796 5892 4392 9701 1941 5212 4532 3988 3848 8068 4120 3983 3572 3162\n",
      " 3980 4526 4256 4933 3168 4524 5478 9282 3036 3439 6155 9719 7127 3164\n",
      " 7797 2892 9711 3576 8341 7117 8612 4523 3574 5070 8759 8065 4932 3435\n",
      " 2619 9565 7380 9697 8337 6018 7252 6977 1669 3163 8469 7793 8473 4260\n",
      " 7249 7385 6709 8756 8749 1807 7135 2896 7521 4391 3028 5748 4804 5202\n",
      " 1948 9146 7815 9017 2079 8069 9293 7389 2628 3852 6705 8359 5208 6981\n",
      " 2890 3570 2220 5076 5753 5615 7111 6297 7807 9303 6834 6573 2620 7668\n",
      " 2349 4255 3030 7113 5757 2077 6023 8484 9157 9428 5889 9300 9583 3031\n",
      " 9712 2622 3979 7671 9575 1676 3166 9561 3984 7532 6727 6430 6569 9167\n",
      " 2894 2755 6855 8767 2759 5073 7253 5338 8351 9425 2487 9708] 404\n"
     ]
    }
   ],
   "source": [
    "## get frequent important feature set\n",
    "itemsets_y = itm.transform_intgrad_to_itemsets(y_int_g,thd=thd)\n",
    "fids = itm.gen_freq_feature_set(itemsets_y[0],min_support=100,max_len=500)\n",
    "fids = np.array(fids).astype(int)-1\n",
    "print('feature set',fids,len(fids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6734d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters\n",
    "\n",
    "grid_search = False\n",
    "\n",
    "if grid_search:\n",
    "    ng_range = np.arange(2,11)\n",
    "    bin_strategies = [\"uniform\",\"kmeans\"]\n",
    "    support_range = np.arange(1000,10000,1000)\n",
    "    confidence_lower_bound = 0.8\n",
    "    max_depth=2\n",
    "    top_K=3\n",
    "\n",
    "\n",
    "    best_rule_set,best_configs,config_metric_records = rlm.param_grid_search_for_amore(bin_strategies,ng_range,support_range,X=x,fids=fids,target_indices=(pred_y>y_thd),y=y_train,c=1,confidence_lower_bound=confidence_lower_bound,\n",
    "                                                                                        max_depth=max_depth,top_K=top_K,sort_by=\"fitness\")\n",
    "    print(best_rule_set,best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc57debd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### search rules for target pattern: pred_y > y_thd  ###\n",
    "### we set the hyperparameters obtaind by above grid search step ###\n",
    "min_support=2000\n",
    "num_grids=7\n",
    "max_depth=2\n",
    "bin_strategy=\"uniform\"\n",
    "\n",
    "x = X_train_raw.reshape(X_train_raw.shape[0],-1).numpy()\n",
    "y_rule_candidates = rlm.gen_rule_list_for_one_target(x,fids,pred_y>y_thd,y=y_train,c=1,sort_by=\"fitness\",\n",
    "                                                    min_support=min_support,num_grids=num_grids,max_depth=max_depth,top_K=3,\n",
    "                                                    local_x=None,feature_types=feature_types*time_len,bin_strategy=bin_strategy,\n",
    "                                                    verbose=False,search=\"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "559cd7f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rules': [(6154, 'HR_ctime_t45', '>=', 34.0),\n",
       "   (5204, 'Temp_ctime_t38', '>=', 34.0)],\n",
       "  'confidence': 0.8597678916827853,\n",
       "  'support': 2068,\n",
       "  'fitness': 0.20476124948396865,\n",
       "  'cond_prob_y': 0.23597678916827852,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(6154, 'HR_ctime_t45', '>=', 34.0),\n",
       "   (5612, 'Temp_ctime_t41', '>=', 37.0)],\n",
       "  'confidence': 0.8601161665053243,\n",
       "  'support': 2066,\n",
       "  'fitness': 0.20476124948396862,\n",
       "  'cond_prob_y': 0.2362052274927396,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(6154, 'HR_ctime_t45', '>=', 34.0),\n",
       "   (5476, 'Temp_ctime_t40', '>=', 36.0)],\n",
       "  'confidence': 0.8601161665053243,\n",
       "  'support': 2066,\n",
       "  'fitness': 0.20476124948396862,\n",
       "  'cond_prob_y': 0.2362052274927396,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5338, 'HR_ctime_t39', '>=', 29.0),\n",
       "   (7652, 'Temp_ctime_t56', '>=', 49.0)],\n",
       "  'confidence': 0.8275024295432458,\n",
       "  'support': 2058,\n",
       "  'fitness': 0.1854960781615522,\n",
       "  'cond_prob_y': 0.2371234207968902,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5474, 'HR_ctime_t40', '>=', 30.0),\n",
       "   (7652, 'Temp_ctime_t56', '>=', 49.0)],\n",
       "  'confidence': 0.8274185707340788,\n",
       "  'support': 2057,\n",
       "  'fitness': 0.18535846979496354,\n",
       "  'cond_prob_y': 0.23723869713174525,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5338, 'HR_ctime_t39', '>=', 29.0),\n",
       "   (7788, 'Temp_ctime_t57', '>=', 50.0)],\n",
       "  'confidence': 0.8274185707340788,\n",
       "  'support': 2057,\n",
       "  'fitness': 0.18535846979496354,\n",
       "  'cond_prob_y': 0.23723869713174525,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5338, 'HR_ctime_t39', '>=', 29.0),\n",
       "   (7924, 'Temp_ctime_t58', '>=', 51.0)],\n",
       "  'confidence': 0.8274185707340788,\n",
       "  'support': 2057,\n",
       "  'fitness': 0.18535846979496354,\n",
       "  'cond_prob_y': 0.23723869713174525,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5474, 'HR_ctime_t40', '>=', 30.0),\n",
       "   (7788, 'Temp_ctime_t57', '>=', 50.0)],\n",
       "  'confidence': 0.8273346303501945,\n",
       "  'support': 2056,\n",
       "  'fitness': 0.18522086142837482,\n",
       "  'cond_prob_y': 0.23735408560311283,\n",
       "  'ratio_y': 0.27743035815804434},\n",
       " {'rules': [(5474, 'HR_ctime_t40', '>=', 30.0),\n",
       "   (7924, 'Temp_ctime_t58', '>=', 51.0)],\n",
       "  'confidence': 0.8273346303501945,\n",
       "  'support': 2056,\n",
       "  'fitness': 0.18522086142837482,\n",
       "  'cond_prob_y': 0.23735408560311283,\n",
       "  'ratio_y': 0.27743035815804434}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [f+\"_t\"+str(t) for t in range(time_len) for f in input_feature_names ]\n",
    "for i, rules in enumerate(y_rule_candidates):   \n",
    "    rules[\"rules\"] = rlm.replace_feature_names(rules[\"rules\"],feature_names)\n",
    "    y_rule_candidates[i] = rules\n",
    "y_rule_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0337991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c40438dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters for Decision Tree Classifier\n",
    "\n",
    "if grid_search:\n",
    "\n",
    "    criteria = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    support_range = np.arange(1000,10000,1000)\n",
    "    w = (pred_y>y_thd).sum()/pred_y.shape[0]\n",
    "    class_weight_options = [{0:0.5,1:0.5},{0:1./(1.-w),1:1./w}]\n",
    "\n",
    "    DT_best_rule_set, DT_best_configs, DT_config_metric_records = dtr.param_grid_search_for_DT(criteria,support_range,weight_options=class_weight_options,X=X_tr,y=y_train,target_indices=pred_y>y_thd,c=1,max_depth=max_depth,feature_names=feature_names,confidence_lower_bound=confidence_lower_bound,seed=seed)\n",
    "    print(DT_best_rule_set, DT_best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42de650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(class_weight={0: 0.5, 1: 0.5}, max_depth=2,\n",
       "                       min_samples_leaf=2000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight={0: 0.5, 1: 0.5}, max_depth=2,\n",
       "                       min_samples_leaf=2000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(class_weight={0: 0.5, 1: 0.5}, max_depth=2,\n",
       "                       min_samples_leaf=2000, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Obtain rules for target pattern: pred_y > y_thd from a DecisionTreeClassifier ###\n",
    "### We set the hyperparameters obtaind by above grid search step ###\n",
    "\n",
    "criterion = \"gini\"\n",
    "min_support = 2000\n",
    "class_weight = {0: 0.5, 1: 0.5}\n",
    "\n",
    "X_tr = X_train_raw.reshape(X_train_raw.shape[0],-1).numpy()\n",
    "X_tr[np.isnan(X_tr)] = 0.\n",
    "treemodel = DecisionTreeClassifier(max_depth=max_depth,min_samples_leaf=min_support,random_state=seed,criterion=criterion,class_weight=class_weight)\n",
    "treemodel.fit(X_tr,pred_y>y_thd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09cf064e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_3029 <= 9.50\n",
      "|   |--- feature_9566 <= 70.50\n",
      "|   |   |--- class: False\n",
      "|   |--- feature_9566 >  70.50\n",
      "|   |   |--- class: False\n",
      "|--- feature_3029 >  9.50\n",
      "|   |--- feature_2892 <= 18.50\n",
      "|   |   |--- class: True\n",
      "|   |--- feature_2892 >  18.50\n",
      "|   |   |--- class: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(export_text(treemodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef53d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_list, rule_value_list, rule_metric_list, new_lines = dtr.obtain_rule_lists_from_DT(treemodel,X_tr,y_train,pred_y>y_thd,np.arange(X_tr.shape[-1]),feature_names,c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f674808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################\n",
      "[(3029, '<=', 9.5), (9566, '<=', 70.5)]\n",
      "confidence 0.295 cond_prob_y 0.059 support 13285 fitness -0.75\n",
      "SBP_ctime_t22 <= 9.5\n",
      "PaCO2_ctime_t70 <= 70.5\n",
      "#################\n",
      "[(3029, '<=', 9.5), (9566, '>', 70.5)]\n",
      "confidence 0.021 cond_prob_y 0.016 support 14376 fitness -1.894\n",
      "SBP_ctime_t22 <= 9.5\n",
      "PaCO2_ctime_t70 > 70.5\n",
      "#################\n",
      "[(3029, '>', 9.5), (2892, '<=', 18.5)]\n",
      "confidence 0.519 cond_prob_y 0.11 support 2605 fitness 0.014\n",
      "SBP_ctime_t22 > 9.5\n",
      "Temp_ctime_t21 <= 18.5\n",
      "#################\n",
      "[(3029, '>', 9.5), (2892, '>', 18.5)]\n",
      "confidence 0.846 cond_prob_y 0.226 support 2000 fitness 0.19\n",
      "SBP_ctime_t22 > 9.5\n",
      "Temp_ctime_t21 > 18.5\n"
     ]
    }
   ],
   "source": [
    "dtr.display_rules_from_DT(rule_list,rule_metric_list,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0261e",
   "metadata": {},
   "source": [
    "### Below code is for plotting confidence and fitness while varying minimum support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de81b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need grid search first to get the results with different minimum supports\n",
    "if grid_search:\n",
    "    cf_mtx = np.vstack([config_metric_records[key][\"top_confidence_records\"] for key in config_metric_records.keys()])\n",
    "    ft_mtx = np.vstack([config_metric_records[key][\"top_fitness_records\"] for key in config_metric_records.keys()])\n",
    "    as_mtx = np.vstack([config_metric_records[key][\"actual_support\"] for key in config_metric_records.keys()])\n",
    "\n",
    "\n",
    "    DT_cf_mtx = np.vstack([DT_config_metric_records[key][\"top_confidence_records\"] for key in DT_config_metric_records.keys()])\n",
    "    DT_ft_mtx = np.vstack([DT_config_metric_records[key][\"top_fitness_records\"] for key in DT_config_metric_records.keys()])\n",
    "    DT_as_mtx = np.vstack([DT_config_metric_records[key][\"actual_support\"] for key in DT_config_metric_records.keys()])\n",
    "    \n",
    "    best_cfs,best_fts,best_ass=[],[],[]\n",
    "    DT_best_cfs,DT_best_fts,DT_best_ass=[],[],[]\n",
    "    ft_mtx_cp = ft_mtx.copy()\n",
    "    DT_ft_mtx_cp = DT_ft_mtx.copy()\n",
    "\n",
    "    ft_mtx_cp[cf_mtx<confidence_lower_bound]=0\n",
    "    DT_ft_mtx_cp[DT_cf_mtx<confidence_lower_bound]=0.\n",
    "\n",
    "    for s in range(cf_mtx.shape[1]):\n",
    "        cid = np.argmax(ft_mtx_cp[:,s])\n",
    "        bc = cf_mtx[cid,s]\n",
    "        if bc >= confidence_lower_bound:\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(ft_mtx[:,s])\n",
    "            bc = cf_mtx[cid,s]\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "\n",
    "\n",
    "        cid = np.argmax(DT_ft_mtx_cp[:,s])\n",
    "        bc = DT_cf_mtx[cid,s]\n",
    "        if bc >= confidence_lower_bound:\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(DT_ft_mtx[:,s])\n",
    "            bc = DT_cf_mtx[cid,s]\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3043b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search:\n",
    "    import seaborn as sns\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(5,4))\n",
    "\n",
    "    color1 = '#377eb8'  # Blue\n",
    "    color2 = '#e41a1c'  # Red\n",
    "    color3 = '#4daf4a'  # green\n",
    "\n",
    "    plt.plot(support_range,best_cfs,\"-o\",color=color1,markersize=4)\n",
    "    plt.plot(support_range,best_fts,\"--o\",color=color1,markersize=4)\n",
    "\n",
    "    plt.plot(support_range,DT_best_cfs,\"-o\",color=color2,markersize=4)\n",
    "    plt.plot(support_range,DT_best_fts,\"--o\",color=color2,markersize=4)\n",
    "\n",
    "    plt.xlim(1000,7000)\n",
    "    plt.ylim(0.,1.)\n",
    "\n",
    "    # Creating custom lines for the color legend\n",
    "    custom_lines_color = [Line2D([0], [0], color=color1, lw=4),\n",
    "                          Line2D([0], [0], color=color2, lw=4)]\n",
    "\n",
    "\n",
    "    # Creating custom lines for the line style legend\n",
    "    custom_lines_style = [Line2D([0], [0], color='grey', lw=2, linestyle='-'),\n",
    "                          Line2D([0], [0], color='grey', lw=2, linestyle='--')]\n",
    "\n",
    "    # Creating the color legend\n",
    "    color_legend = plt.legend(custom_lines_color, ['AMORE', 'DT Classifier'], loc=[0.07,0.32], title=\"Methods\")\n",
    "\n",
    "    # Adding the color legend manually to avoid it being replaced by the line style legend\n",
    "    plt.gca().add_artist(color_legend)\n",
    "\n",
    "    # Creating the line style legend\n",
    "    plt.legend(custom_lines_style, ['Confidence','Fitness'], loc=[0.58,0.32], title=\"Metrics\")\n",
    "    plt.xlabel(\"Specified minimum support\")\n",
    "    plt.savefig(\"./results/sepsis/compare_DT.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5c0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576dcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
