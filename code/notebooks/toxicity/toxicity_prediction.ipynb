{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "here = os.getcwd()\n",
    "sys.path.append(os.path.join(here,\"../../\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import explainer.rule_pattern_miner as rlm\n",
    "import explainer.DT_rules as dtr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_feature_raw_value(fid,fval,featur_names,raw_min_max,ntype=\"min_max\"):\n",
    "    fn = featur_names[fid]\n",
    "    if ntype == \"min_max\":\n",
    "        if fn in raw_min_max.columns:\n",
    "            mx = raw_min_max.loc[:,fn].max()\n",
    "            mi = raw_min_max.loc[:,fn].min()\n",
    "            return fval*(mx-mi)+mi\n",
    "        return fval\n",
    "    else:\n",
    "        raise TypeError(\"Not yet supported type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Control randomness for reproducibility\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Tox21(7831):\n",
      "====================\n",
      "Number of graphs: 7831\n",
      "Number of features: 9\n",
      "Number of classes: 12\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "dataset = MoleculeNet(root='../../../data/MoleculeNet', name='Tox21')\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transform SMILES to RDKit descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None  # Handle invalid SMILES input\n",
    "\n",
    "    descriptor_name = []; value_list = []\n",
    "    for name, descriptor_function in Descriptors.descList:\n",
    "        value = descriptor_function(mol)\n",
    "        value_list.append(value)\n",
    "        descriptor_name.append(name)\n",
    "\n",
    "    return value_list, descriptor_name\n",
    "\n",
    "def smiles_to_rdkit(dataset):\n",
    "    rdk_features = []\n",
    "    for i in range(len(dataset)):\n",
    "        smiles = dataset[i].smiles\n",
    "        features, descriptors = calculate_all_descriptors(smiles)\n",
    "        rdk_features.append(features)\n",
    "\n",
    "    return torch.tensor(rdk_features, dtype=torch.float32), descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Divide the data into training validatiton and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:38:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:38:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:38:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 5481\n",
      "Number of validation graphs: 783\n",
      "Number of test graphs: 1567\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:int(len(dataset) * 0.7)]\n",
    "val_dataset = dataset[int(len(dataset) * 0.7):int(len(dataset) * 0.8)]\n",
    "test_dataset = dataset[int(len(dataset) * 0.8):]\n",
    "\n",
    "## generate rdk descriptors from smiles representation of molecules\n",
    "dataset_rdk, descriptors = smiles_to_rdkit(dataset)\n",
    "## remove descriptors with nan values\n",
    "descriptors = [descriptors[i] for i in range(len(descriptors)) if ~torch.any(dataset_rdk.isnan(), dim=0)[i]]\n",
    "dataset_rdk = dataset_rdk[:,~torch.any(dataset_rdk.isnan(), dim=0)]\n",
    "dataset_rdk = (dataset_rdk - dataset_rdk.mean(axis=0))/dataset_rdk.std(axis=0) ## normalize\n",
    "descriptors = [descriptors[i] for i in range(len(descriptors)) if ~torch.any(dataset_rdk.isnan(), dim=0)[i]]\n",
    "dataset_rdk = dataset_rdk[:,~torch.any(dataset_rdk.isnan(), dim=0)]\n",
    "\n",
    "train_dataset_rdk = dataset_rdk[:int(len(dataset_rdk) * 0.7)]\n",
    "val_dataset_rdk = dataset_rdk[int(len(dataset_rdk) * 0.7):int(len(dataset_rdk) * 0.8)]\n",
    "test_dataset_rdk = dataset_rdk[int(len(dataset_rdk) * 0.8):]\n",
    "\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "## task weights for training\n",
    "mask = 1-torch.isnan(train_dataset.y).type(torch.float32)\n",
    "task_weight = 1 / torch.mean(mask, axis = 0)\n",
    "label_weight = torch.sum(train_dataset.y == 0., axis = 0) / torch.sum(train_dataset.y == 1., axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64; learning_rate=0.001; num_epoch=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.activation_fn_last = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.activation_fn_last(self.lin(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GNNtraining(object):\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 learning_rate=0.001, \n",
    "                 num_epoch=200,\n",
    "                 use_cuda=False):\n",
    "        \n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        if use_cuda:\n",
    "            self.model.cuda()\n",
    "        \n",
    "    def training(self, train_loader, val_loader, task_weight, label_weight):     \n",
    "        parameters = set(self.model.parameters())\n",
    "        optimizer = optim.Adam(parameters, lr=self.learning_rate, eps=1e-3)\n",
    "\n",
    "        for epoch in range(self.num_epoch):\n",
    "            for data in train_loader:\n",
    "                y_batch = torch.nan_to_num(data.y, nan=0.0) # nan to 0.0\n",
    "                \n",
    "                task_weight_batch = ~data.y.isnan()*task_weight # weight each task according to the number of valid labels\n",
    "                label_weight_batch = y_batch * label_weight; label_weight_batch[label_weight_batch==0.0] = 1.0 # weight each label according to the number of positive labels\n",
    "                w_batch = task_weight_batch * label_weight_batch\n",
    "                if self.use_cuda:\n",
    "                    data = data.cuda(); y_batch = y_batch.cuda(); w_batch = w_batch.cuda()\n",
    "                criterion = nn.BCELoss(weight=w_batch)\n",
    "                optimizer.zero_grad()\n",
    "                self.model.train()\n",
    "                # calculate the training loss\n",
    "                output = self.model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_bce = loss.detach()\n",
    "            train_acc = self.evaluation(train_loader)\n",
    "            val_acc = self.evaluation(val_loader)\n",
    "            print('>>> Epoch {:5d}/{:5d} | train_bce={:.5f} | train_acc={:.5f} | val_acc={:.5f}'.format(epoch, self.num_epoch, train_bce, train_acc, val_acc))\n",
    "                \n",
    "    def evaluation(self, loader):\n",
    "        self.model.eval()\n",
    "        correct = 0; total = 0\n",
    "        for data in loader:\n",
    "            if self.use_cuda:\n",
    "                data = data.cuda()\n",
    "            output = self.model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "            pred = (output > 0.5).to(torch.float32)\n",
    "            correct += int((pred == data.y).sum())\n",
    "            total += int((~data.y.isnan()).sum())\n",
    "\n",
    "        return correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GDataLoader\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "train_loader = GDataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = GDataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "gnn = GNN(hidden_channels=64)\n",
    "\n",
    "## If train the model from scratch, change it a non-existing path \n",
    "model_path = 'gnn_models/gnn.pt' \n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    gnn.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    gnn_training = GNNtraining(gnn, learning_rate, num_epoch, use_cuda=False)\n",
    "    gnn_training.training(train_loader, val_loader, task_weight, label_weight)\n",
    "    torch.save(gnn.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader_visualization = GDataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "for train_data in train_loader_visualization:\n",
    "    break\n",
    "for val_data in val_loader:\n",
    "    break\n",
    "train_loader_rdk_visualization = DataLoader(TensorDataset(train_dataset_rdk, train_dataset.y), batch_size=len(train_dataset_rdk), shuffle=False)\n",
    "for train_data_x, train_data_y in train_loader_rdk_visualization:\n",
    "    break\n",
    "\n",
    "y_pred_s_gnn = gnn(train_data.x.to(torch.float32), train_data.edge_index, train_data.batch).detach().numpy()\n",
    "\n",
    "y_pred_test_gnn = gnn(val_data.x.to(torch.float32), val_data.edge_index, val_data.batch).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Task1 -- AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: total positive: 219; total predictived positive:834; recall:0.73059; precision:0.19185; auroc: 0.88678; test auroc:  0.57895\n"
     ]
    }
   ],
   "source": [
    "idx_task = 0\n",
    "\n",
    "y_true = train_data.y.numpy()[:,idx_task:idx_task+1].reshape(-1); idx = ~np.isnan(y_true)\n",
    "y_pred_s = y_pred_s_gnn[:,idx_task:idx_task+1].reshape(-1)[idx]\n",
    "y_true = y_true[idx].astype(int)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_s)\n",
    "th_id = np.argmax(tpr - fpr); y_thd = thresholds[th_id]\n",
    "\n",
    "fids = [i for i in range(len(descriptors))]\n",
    "feature_types = []\n",
    "for i in range(len(descriptors)):\n",
    "    c = descriptors[i]\n",
    "    if np.dtype(train_data_x.numpy()[idx][:,i][0])!=np.uint8:\n",
    "        feature_types.append(str(np.dtype(train_data_x.numpy()[idx][:,i][0])))\n",
    "    else:\n",
    "        feature_types.append(\"cat\")\n",
    "    # print(c,np.dtype(train_data_x.numpy()[idx][:,i][0]),feature_types[-1])\n",
    "\n",
    "y_true_test = val_data.y.numpy()[:,idx_task:idx_task+1].reshape(-1); idx_test = ~np.isnan(y_true_test)\n",
    "y_pred_test = y_pred_test_gnn[:,idx_task:idx_task+1].reshape(-1)[idx_test]\n",
    "y_true_test = y_true_test[idx_test].astype(int)\n",
    "\n",
    "print('Task {}: total positive: {:}; total predictived positive:{:}; recall:{:.5f}; precision:{:.5f}; auroc: {:.5f}; test auroc:  {:.5f}'.format(idx_task, sum(y_true), sum(y_pred_s>y_thd), recall_score(y_true, y_pred_s>y_thd), precision_score(y_true, y_pred_s>y_thd), roc_auc_score(y_true, y_pred_s), roc_auc_score(y_true_test, y_pred_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters\n",
    "\n",
    "grid_search = False\n",
    "\n",
    "if grid_search:\n",
    "    ng_range = np.arange(2,11)\n",
    "    bin_strategies = [\"uniform\",\"kmeans\"]\n",
    "    support_range = np.arange(100,800,100)\n",
    "    confidence_lower_bound = 0.8\n",
    "    max_depth=2\n",
    "    top_K=3\n",
    "\n",
    "\n",
    "    best_rule_set,best_configs,config_metric_records = rlm.param_grid_search_for_amore(bin_strategies,ng_range,support_range,train_data_x.numpy()[idx],fids,target_indices=y_pred_s>y_thd,y=y_true,c=1,confidence_lower_bound = confidence_lower_bound,\n",
    "                                                                                        max_depth=max_depth,top_K=top_K,sort_by=\"fitness\")\n",
    "    print(best_rule_set,best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rules': [(95, 'NumAliphaticRings', '>=', 1.1488014),\n",
       "   (95, 'NumAliphaticRings', '<=', 10.405089),\n",
       "   (52, 'SMR_VSA5', '>=', 0.16672008)],\n",
       "  'confidence': 0.8006134969325154,\n",
       "  'support': 326,\n",
       "  'fitness': 0.2350119904076739,\n",
       "  'cond_prob_y': 0.34355828220858897,\n",
       "  'ratio_y': 0.5114155251141552},\n",
       " {'rules': [(95, 'NumAliphaticRings', '>=', 1.1488014),\n",
       "   (95, 'NumAliphaticRings', '<=', 10.405089),\n",
       "   (16, 'Chi0', '>=', 0.065092325)],\n",
       "  'confidence': 0.7689969604863222,\n",
       "  'support': 329,\n",
       "  'fitness': 0.2122302158273381,\n",
       "  'cond_prob_y': 0.3282674772036474,\n",
       "  'ratio_y': 0.4931506849315068},\n",
       " {'rules': [(95, 'NumAliphaticRings', '>=', 1.1488014),\n",
       "   (95, 'NumAliphaticRings', '<=', 10.405089),\n",
       "   (124, 'fr_NH0', '<=', 0.5092023)],\n",
       "  'confidence': 0.7731629392971247,\n",
       "  'support': 313,\n",
       "  'fitness': 0.2050359712230216,\n",
       "  'cond_prob_y': 0.3514376996805112,\n",
       "  'ratio_y': 0.502283105022831},\n",
       " {'rules': [(26, 'Chi4n', '>=', 1.6014603),\n",
       "   (12, 'FpDensityMorgan3', '>=', -0.9372277)],\n",
       "  'confidence': 0.7443365695792881,\n",
       "  'support': 309,\n",
       "  'fitness': 0.1810551558752998,\n",
       "  'cond_prob_y': 0.3300970873786408,\n",
       "  'ratio_y': 0.4657534246575342},\n",
       " {'rules': [(26, 'Chi4n', '>=', 1.6014603),\n",
       "   (64, 'SlogP_VSA6', '>=', -1.0870111),\n",
       "   (64, 'SlogP_VSA6', '<=', 1.8963256)],\n",
       "  'confidence': 0.745928338762215,\n",
       "  'support': 307,\n",
       "  'fitness': 0.18105515587529977,\n",
       "  'cond_prob_y': 0.3289902280130293,\n",
       "  'ratio_y': 0.4611872146118721},\n",
       " {'rules': [(26, 'Chi4n', '>=', 1.6014603),\n",
       "   (3, 'MinEStateIndex', '>=', -1.1439164)],\n",
       "  'confidence': 0.7373417721518988,\n",
       "  'support': 316,\n",
       "  'fitness': 0.17985611510791372,\n",
       "  'cond_prob_y': 0.310126582278481,\n",
       "  'ratio_y': 0.4474885844748858},\n",
       " {'rules': [(145, 'fr_bicyclic', '>=', 1.3638039),\n",
       "   (145, 'fr_bicyclic', '<=', 5.621136),\n",
       "   (59, 'SlogP_VSA12', '<=', 0.58404183)],\n",
       "  'confidence': 0.696078431372549,\n",
       "  'support': 306,\n",
       "  'fitness': 0.14388489208633093,\n",
       "  'cond_prob_y': 0.3431372549019608,\n",
       "  'ratio_y': 0.4794520547945205},\n",
       " {'rules': [(145, 'fr_bicyclic', '>=', 1.3638039),\n",
       "   (145, 'fr_bicyclic', '<=', 5.621136),\n",
       "   (37, 'PEOE_VSA13', '<=', 1.4442309)],\n",
       "  'confidence': 0.6923076923076923,\n",
       "  'support': 312,\n",
       "  'fitness': 0.14388489208633093,\n",
       "  'cond_prob_y': 0.34294871794871795,\n",
       "  'ratio_y': 0.4885844748858447},\n",
       " {'rules': [(145, 'fr_bicyclic', '>=', 1.3638039),\n",
       "   (145, 'fr_bicyclic', '<=', 5.621136),\n",
       "   (76, 'EState_VSA7', '<=', 1.7982086)],\n",
       "  'confidence': 0.695364238410596,\n",
       "  'support': 302,\n",
       "  'fitness': 0.14148681055155876,\n",
       "  'cond_prob_y': 0.3509933774834437,\n",
       "  'ratio_y': 0.4840182648401826}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### search rules for target pattern: pred_y > y_thd  ###\n",
    "### we set the hyperparameters obtaind by above grid search step ###\n",
    "min_support=300\n",
    "num_grids=3\n",
    "max_depth=2\n",
    "bin_strategy=\"kmeans\"\n",
    "top_K = 3\n",
    "\n",
    "y_rule_candidates = rlm.gen_rule_list_for_one_target(train_data_x.numpy()[idx],fids,y_pred_s>y_thd,y=y_true,c=1,sort_by=\"fitness\",\n",
    "                                                    min_support=min_support,num_grids=num_grids,max_depth=max_depth,top_K=top_K,\n",
    "                                                    local_x=None,bin_strategy=bin_strategy,\n",
    "                                                    verbose=False,search=\"greedy\")\n",
    "for i, rules in enumerate(y_rule_candidates):   \n",
    "    rules[\"rules\"] = rlm.replace_feature_names(rules[\"rules\"],descriptors)\n",
    "    y_rule_candidates[i] = rules\n",
    "    \n",
    "y_rule_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters\n",
    "\n",
    "if grid_search:\n",
    "    criteria = [\"gini\", \"entropy\", \"log_loss\"]; c=1.\n",
    "    w = (y_true==c).sum()/y_true.shape[0]\n",
    "    # class_weight_options = [{0:0.5,1:0.5},{0:1./(1.-w),1:1./w}]\n",
    "    class_weight_options = [{0:0.5,1:0.5},'balanced']\n",
    "    support_range = np.arange(100,800,100)\n",
    "    \n",
    "    ## If no any rule sets above 0.8, set it to 0. to simply select by fitness\n",
    "    confidence_lower_bound = 0.8 \n",
    "    max_depth=2\n",
    "    DT_best_rule_set, DT_best_configs, DT_config_metric_records = dtr.param_grid_search_for_DT(criteria,support_range,weight_options=class_weight_options,X=train_data_x.numpy()[idx],y=y_true,target_indices=y_pred_s>y_thd,c=1,max_depth=max_depth,feature_names=descriptors,confidence_lower_bound=confidence_lower_bound,seed=seed)\n",
    "    print(DT_best_rule_set, DT_best_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_95 <= 1.41\n",
      "|   |--- feature_145 <= 0.70\n",
      "|   |   |--- class: False\n",
      "|   |--- feature_145 >  0.70\n",
      "|   |   |--- class: False\n",
      "|--- feature_95 >  1.41\n",
      "|   |--- feature_52 <= 0.84\n",
      "|   |   |--- class: False\n",
      "|   |--- feature_52 >  0.84\n",
      "|   |   |--- class: True\n",
      "\n",
      "#################\n",
      "[(95, '<=', 1.414), (145, '<=', 0.698)]\n",
      "confidence 0.097 cond_prob_y 0.019 support 4380 fitness -4.235\n",
      "NumAliphaticRings <= 1.414\n",
      "fr_bicyclic <= 0.698\n",
      "#################\n",
      "[(95, '<=', 1.414), (145, '>', 0.698)]\n",
      "confidence 0.431 cond_prob_y 0.058 support 311 fitness -0.052\n",
      "NumAliphaticRings <= 1.414\n",
      "fr_bicyclic > 0.698\n",
      "#################\n",
      "[(95, '>', 1.414), (52, '<=', 0.836)]\n",
      "confidence 0.407 cond_prob_y 0.096 support 135 fitness -0.03\n",
      "NumAliphaticRings > 1.414\n",
      "SMR_VSA5 <= 0.836\n",
      "#################\n",
      "[(95, '>', 1.414), (52, '>', 0.836)]\n",
      "confidence 0.874 cond_prob_y 0.407 support 253 fitness 0.227\n",
      "NumAliphaticRings > 1.414\n",
      "SMR_VSA5 > 0.836\n"
     ]
    }
   ],
   "source": [
    "### Obtain rules for target pattern: pred_y > y_thd from a DecisionTreeClassifier ###\n",
    "### We set the hyperparameters obtaind by above grid search step ###\n",
    "criterion=\"gini\"\n",
    "min_support=100\n",
    "class_weight={0: 0.5, 1: 0.5}\n",
    "\n",
    "input_feature_names =descriptors\n",
    "treemodel = DecisionTreeClassifier(max_depth=max_depth,min_samples_leaf=min_support,criterion=criterion,random_state=seed,class_weight=class_weight)\n",
    "treemodel.fit(train_data_x.numpy()[idx],y_pred_s>y_thd)\n",
    "rule_list, rule_value_list, rule_metric_list, new_lines = dtr.obtain_rule_lists_from_DT(treemodel,train_data_x.numpy()[idx],y_true,y_pred_s>y_thd,np.arange(train_data_x.numpy()[idx].shape[-1]),descriptors,c=1)\n",
    "print(export_text(treemodel))\n",
    "\n",
    "## display rules extracted by DT classifier\n",
    "dtr.display_rules_from_DT(rule_list,rule_metric_list,input_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 AR: fitness and confidence with different minimum support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to do grid search first\n",
    "if grid_search:\n",
    "    confidence_lower_bound=0.8\n",
    "\n",
    "    cf_mtx = np.vstack([config_metric_records[key]['top_confidence_records'] for key in config_metric_records.keys()])\n",
    "    ft_mtx = np.vstack([config_metric_records[key]['top_fitness_records'] for key in config_metric_records.keys()])\n",
    "    as_mtx = np.vstack([config_metric_records[key]['actual_support'] for key in config_metric_records.keys()])\n",
    "    DT_cf_mtx = np.vstack([DT_config_metric_records[key]['top_confidence_records'] for key in DT_config_metric_records.keys()])\n",
    "    DT_ft_mtx = np.vstack([DT_config_metric_records[key]['top_fitness_records'] for key in DT_config_metric_records.keys()])\n",
    "    DT_as_mtx = np.vstack([DT_config_metric_records[key]['actual_support'] for key in DT_config_metric_records.keys()])\n",
    "    best_cfs,best_fts,best_ass=[],[],[]\n",
    "    DT_best_cfs,DT_best_fts,DT_best_ass=[],[],[]\n",
    "    ft_mtx_cp = ft_mtx.copy()\n",
    "    DT_ft_mtx_cp = DT_ft_mtx.copy()\n",
    "\n",
    "    ft_mtx_cp[cf_mtx<confidence_lower_bound]=0\n",
    "    DT_ft_mtx_cp[DT_cf_mtx<confidence_lower_bound]=0.\n",
    "\n",
    "    for s in range(cf_mtx.shape[1]):\n",
    "        cid = np.argmax(ft_mtx_cp[:,s])\n",
    "\n",
    "        bc = cf_mtx[cid,s]\n",
    "        print(cid,bc)\n",
    "        if bc >= confidence_lower_bound:\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(ft_mtx[:,s])\n",
    "            bc = cf_mtx[cid,s]\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "\n",
    "\n",
    "        cid = np.argmax(DT_ft_mtx_cp[:,s])\n",
    "        bc = DT_cf_mtx[cid,s]\n",
    "        print(\"DT\",cid,bc)\n",
    "        if bc >= confidence_lower_bound:\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(DT_ft_mtx[:,s])\n",
    "            bc = DT_cf_mtx[cid,s]\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "\n",
    "\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(5,4))\n",
    "    color1 = '#377EB8'  # Blue\n",
    "    color2 = '#E41A1C'  # Red\n",
    "    color3 = '#4DAF4A'  # green\n",
    "    plt.plot(support_range,best_cfs,'-o',color=color1,markersize=4)\n",
    "    plt.plot(support_range,best_fts,'--o',color=color1,markersize=4)\n",
    "    plt.plot(support_range,DT_best_cfs,'-o',color=color2,markersize=4)\n",
    "    plt.plot(support_range,DT_best_fts,'--o',color=color2,markersize=4)\n",
    "    plt.xlim(100,700)\n",
    "    plt.ylim(-0.2,1.)\n",
    "    # Creating custom lines for the color legend\n",
    "    custom_lines_color = [Line2D([0], [0], color=color1, lw=4),\n",
    "                          Line2D([0], [0], color=color2, lw=4)]\n",
    "    # Creating custom lines for the line style legend\n",
    "    custom_lines_style = [Line2D([0], [0], color='grey', lw=2, linestyle='-'),\n",
    "                          Line2D([0], [0], color='grey', lw=2, linestyle='--')]\n",
    "\n",
    "    plt.xlabel('Specified minimum support')\n",
    "    plt.savefig('./compare_DT_AR.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Task2 -- AhR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: total positive: 162; total predictived positive:511; recall:0.84568; precision:0.26810; auroc: 0.94816; test auroc:  0.87736\n"
     ]
    }
   ],
   "source": [
    "idx_task = 1\n",
    "\n",
    "y_true = train_data.y.numpy()[:,idx_task:idx_task+1].reshape(-1); idx = ~np.isnan(y_true)\n",
    "y_pred_s = y_pred_s_gnn[:,idx_task:idx_task+1].reshape(-1)[idx]\n",
    "y_true = y_true[idx].astype(int)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_s)\n",
    "th_id = np.argmax(tpr - fpr); y_thd = thresholds[th_id]\n",
    "\n",
    "fids = [i for i in range(len(descriptors))]\n",
    "feature_types = []\n",
    "for i in range(len(descriptors)):\n",
    "    c = descriptors[i]\n",
    "    if np.dtype(train_data_x.numpy()[idx][:,i][0])!=np.uint8:\n",
    "        feature_types.append(str(np.dtype(train_data_x.numpy()[idx][:,i][0])))\n",
    "    else:\n",
    "        feature_types.append(\"cat\")\n",
    "    # print(c,np.dtype(train_data_x.numpy()[idx][:,i][0]),feature_types[-1])\n",
    "\n",
    "y_true_test = val_data.y.numpy()[:,idx_task:idx_task+1].reshape(-1); idx_test = ~np.isnan(y_true_test)\n",
    "y_pred_test = y_pred_test_gnn[:,idx_task:idx_task+1].reshape(-1)[idx_test]\n",
    "y_true_test = y_true_test[idx_test].astype(int)\n",
    "\n",
    "print('Task {}: total positive: {:}; total predictived positive:{:}; recall:{:.5f}; precision:{:.5f}; auroc: {:.5f}; test auroc:  {:.5f}'.format(idx_task, sum(y_true), sum(y_pred_s>y_thd), recall_score(y_true, y_pred_s>y_thd), precision_score(y_true, y_pred_s>y_thd), roc_auc_score(y_true, y_pred_s), roc_auc_score(y_true_test, y_pred_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters\n",
    "\n",
    "grid_search = False\n",
    "\n",
    "if grid_search:\n",
    "    ng_range = np.arange(2,11)\n",
    "    bin_strategies = [\"uniform\",\"kmeans\"]\n",
    "    support_range = np.arange(50,550,50)\n",
    "    confidence_lower_bound = 0.8\n",
    "    max_depth=2\n",
    "    top_K=3\n",
    "\n",
    "\n",
    "    best_rule_set,best_configs,config_metric_records = rlm.param_grid_search_for_amore(bin_strategies,ng_range,support_range,train_data_x.numpy()[idx],fids,target_indices=y_pred_s>y_thd,y=y_true,c=1,confidence_lower_bound = confidence_lower_bound,\n",
    "                                                                                        max_depth=max_depth,top_K=top_K,sort_by=\"fitness\")\n",
    "\n",
    "    print(best_rule_set,best_configs)                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rules': [(93, 'NumAliphaticCarbocycles', '>=', 1.6307147),\n",
       "   (93, 'NumAliphaticCarbocycles', '<=', 4.6790466),\n",
       "   (145, 'fr_bicyclic', '>=', 1.3407482),\n",
       "   (145, 'fr_bicyclic', '<=', 5.643874)],\n",
       "  'confidence': 0.8987341772151899,\n",
       "  'support': 158,\n",
       "  'fitness': 0.2465753424657534,\n",
       "  'cond_prob_y': 0.5443037974683544,\n",
       "  'ratio_y': 0.5308641975308642},\n",
       " {'rules': [(103, 'NumSaturatedCarbocycles', '>=', 1.7713029),\n",
       "   (145, 'fr_bicyclic', '>=', 1.3407482),\n",
       "   (145, 'fr_bicyclic', '<=', 5.643874)],\n",
       "  'confidence': 0.9343065693430657,\n",
       "  'support': 137,\n",
       "  'fitness': 0.23287671232876708,\n",
       "  'cond_prob_y': 0.5766423357664233,\n",
       "  'ratio_y': 0.4876543209876543},\n",
       " {'rules': [(62, 'SlogP_VSA4', '>=', 2.2838614),\n",
       "   (93, 'NumAliphaticCarbocycles', '>=', 1.6307145),\n",
       "   (93, 'NumAliphaticCarbocycles', '<=', 4.679046)],\n",
       "  'confidence': 0.8992248062015504,\n",
       "  'support': 129,\n",
       "  'fitness': 0.2015655577299413,\n",
       "  'cond_prob_y': 0.5426356589147286,\n",
       "  'ratio_y': 0.43209876543209874},\n",
       " {'rules': [(93, 'NumAliphaticCarbocycles', '>=', 1.6307147),\n",
       "   (93, 'NumAliphaticCarbocycles', '<=', 4.6790466),\n",
       "   (62, 'SlogP_VSA4', '>=', 2.2838614)],\n",
       "  'confidence': 0.8992248062015504,\n",
       "  'support': 129,\n",
       "  'fitness': 0.2015655577299413,\n",
       "  'cond_prob_y': 0.5426356589147286,\n",
       "  'ratio_y': 0.43209876543209874},\n",
       " {'rules': [(62, 'SlogP_VSA4', '>=', 2.2838614),\n",
       "   (145, 'fr_bicyclic', '>=', 1.3407482),\n",
       "   (145, 'fr_bicyclic', '<=', 5.643874)],\n",
       "  'confidence': 0.9243697478991597,\n",
       "  'support': 119,\n",
       "  'fitness': 0.19765166340508808,\n",
       "  'cond_prob_y': 0.5966386554621849,\n",
       "  'ratio_y': 0.4382716049382716},\n",
       " {'rules': [(103, 'NumSaturatedCarbocycles', '>=', 1.7713029),\n",
       "   (39, 'PEOE_VSA2', '>=', 0.3432073)],\n",
       "  'confidence': 0.9,\n",
       "  'support': 90,\n",
       "  'fitness': 0.14090019569471623,\n",
       "  'cond_prob_y': 0.5777777777777777,\n",
       "  'ratio_y': 0.32098765432098764},\n",
       " {'rules': [(62, 'SlogP_VSA4', '>=', 2.2838614),\n",
       "   (11, 'FpDensityMorgan2', '>=', 0.37170005)],\n",
       "  'confidence': 0.8947368421052632,\n",
       "  'support': 76,\n",
       "  'fitness': 0.11741682974559686,\n",
       "  'cond_prob_y': 0.618421052631579,\n",
       "  'ratio_y': 0.29012345679012347},\n",
       " {'rules': [(103, 'NumSaturatedCarbocycles', '>=', 1.7713029),\n",
       "   (159, 'fr_ketone', '>=', 2.1309748)],\n",
       "  'confidence': 0.9615384615384616,\n",
       "  'support': 52,\n",
       "  'fitness': 0.09393346379647749,\n",
       "  'cond_prob_y': 0.7692307692307693,\n",
       "  'ratio_y': 0.24691358024691357},\n",
       " {'rules': [(93, 'NumAliphaticCarbocycles', '>=', 1.6307147),\n",
       "   (93, 'NumAliphaticCarbocycles', '<=', 4.6790466),\n",
       "   (72, 'EState_VSA3', '>=', 1.4238961)],\n",
       "  'confidence': 0.8813559322033898,\n",
       "  'support': 59,\n",
       "  'fitness': 0.08806262230919765,\n",
       "  'cond_prob_y': 0.4576271186440678,\n",
       "  'ratio_y': 0.16666666666666666}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### search rules for target pattern: pred_y > y_thd  ###\n",
    "### we set the hyperparameters obtaind by above grid search step ###\n",
    "min_support=50\n",
    "num_grids=3\n",
    "max_depth=2\n",
    "bin_strategy=\"kmeans\"\n",
    "\n",
    "y_rule_candidates = rlm.gen_rule_list_for_one_target(train_data_x.numpy()[idx],fids,y_pred_s>y_thd,y=y_true,c=1,sort_by=\"fitness\",\n",
    "                                                    min_support=min_support,num_grids=num_grids,max_depth=max_depth,top_K=top_K,\n",
    "                                                    local_x=None,bin_strategy=bin_strategy,\n",
    "                                                    verbose=False,search=\"greedy\")\n",
    "for i, rules in enumerate(y_rule_candidates):   \n",
    "    rules[\"rules\"] = rlm.replace_feature_names(rules[\"rules\"],descriptors)\n",
    "    y_rule_candidates[i] = rules\n",
    "    \n",
    "y_rule_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set \"grid_search = True\" to do a grid search for hyperprameters\n",
    "\n",
    "if grid_search:\n",
    "    criteria = [\"gini\", \"entropy\", \"log_loss\"]; c=1.\n",
    "    w = (y_true==c).sum()/y_true.shape[0]\n",
    "    class_weight_options = [{0:0.5,1:0.5},'balanced']\n",
    "    support_range = np.arange(50,550,50)\n",
    "    confidence_lower_bound = 0.8\n",
    "    max_depth=2\n",
    "    DT_best_rule_set, DT_best_configs, DT_config_metric_records = dtr.param_grid_search_for_DT(criteria,support_range,weight_options=class_weight_options,X=train_data_x.numpy()[idx],y=y_true,target_indices=y_pred_s>y_thd,c=1,max_depth=max_depth,feature_names=descriptors,confidence_lower_bound=confidence_lower_bound,seed=seed)\n",
    "    print(DT_best_rule_set, DT_best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_95 <= 1.41\n",
      "|   |--- feature_82 <= 0.29\n",
      "|   |   |--- class: False\n",
      "|   |--- feature_82 >  0.29\n",
      "|   |   |--- class: True\n",
      "|--- feature_95 >  1.41\n",
      "|   |--- feature_62 <= 2.06\n",
      "|   |   |--- class: True\n",
      "|   |--- feature_62 >  2.06\n",
      "|   |   |--- class: True\n",
      "\n",
      "#################\n",
      "[(95, '<=', 1.414), (82, '<=', 0.294)]\n",
      "confidence 0.045 cond_prob_y 0.015 support 3588 fitness -6.391\n",
      "NumAliphaticRings <= 1.414\n",
      "VSA_EState3 <= 0.294\n",
      "#################\n",
      "[(95, '<=', 1.414), (82, '>', 0.294)]\n",
      "confidence 0.17 cond_prob_y 0.018 support 800 fitness -1.033\n",
      "NumAliphaticRings <= 1.414\n",
      "VSA_EState3 > 0.294\n",
      "#################\n",
      "[(95, '>', 1.414), (62, '<=', 2.059)]\n",
      "confidence 0.404 cond_prob_y 0.099 support 161 fitness -0.061\n",
      "NumAliphaticRings > 1.414\n",
      "SlogP_VSA4 <= 2.059\n",
      "#################\n",
      "[(95, '>', 1.414), (62, '>', 2.059)]\n",
      "confidence 0.81 cond_prob_y 0.429 support 184 fitness 0.223\n",
      "NumAliphaticRings > 1.414\n",
      "SlogP_VSA4 > 2.059\n"
     ]
    }
   ],
   "source": [
    "### Obtain rules for target pattern: pred_y > y_thd from a DecisionTreeClassifier ###\n",
    "### We set the hyperparameters obtaind by above grid search step ###\n",
    "criterion=\"gini\"\n",
    "min_support=150\n",
    "class_weight='balanced'\n",
    "\n",
    "input_feature_names =descriptors\n",
    "treemodel = DecisionTreeClassifier(max_depth=max_depth,min_samples_leaf=min_support,criterion=criterion,random_state=seed,class_weight=class_weight)\n",
    "treemodel.fit(train_data_x.numpy()[idx],y_pred_s>y_thd)\n",
    "rule_list, rule_value_list, rule_metric_list, new_lines = dtr.obtain_rule_lists_from_DT(treemodel,train_data_x.numpy()[idx],y_true,y_pred_s>y_thd,np.arange(train_data_x.numpy()[idx].shape[-1]),descriptors,c=1)\n",
    "print(export_text(treemodel))\n",
    "\n",
    "## display rules extracted by DT classifier\n",
    "dtr.display_rules_from_DT(rule_list,rule_metric_list,input_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 AhR: fitness and confidence with different minimum support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to do grid search first\n",
    "if grid_search:\n",
    "    confidence_lower_bound=0.8\n",
    "\n",
    "    cf_mtx = np.vstack([config_metric_records[key]['top_confidence_records'] for key in config_metric_records.keys()])\n",
    "    ft_mtx = np.vstack([config_metric_records[key]['top_fitness_records'] for key in config_metric_records.keys()])\n",
    "    as_mtx = np.vstack([config_metric_records[key]['actual_support'] for key in config_metric_records.keys()])\n",
    "    DT_cf_mtx = np.vstack([DT_config_metric_records[key]['top_confidence_records'] for key in DT_config_metric_records.keys()])\n",
    "    DT_ft_mtx = np.vstack([DT_config_metric_records[key]['top_fitness_records'] for key in DT_config_metric_records.keys()])\n",
    "    DT_as_mtx = np.vstack([DT_config_metric_records[key]['actual_support'] for key in DT_config_metric_records.keys()])\n",
    "    best_cfs,best_fts,best_ass=[],[],[]\n",
    "    DT_best_cfs,DT_best_fts,DT_best_ass=[],[],[]\n",
    "    ft_mtx_cp = ft_mtx.copy()\n",
    "    DT_ft_mtx_cp = DT_ft_mtx.copy()\n",
    "\n",
    "    ft_mtx_cp[cf_mtx<confidence_lower_bound]=0\n",
    "    DT_ft_mtx_cp[DT_cf_mtx<confidence_lower_bound]=0.\n",
    "\n",
    "    for s in range(cf_mtx.shape[1]):\n",
    "        cid = np.argmax(ft_mtx_cp[:,s])\n",
    "\n",
    "        bc = cf_mtx[cid,s]\n",
    "        print(cid,bc)\n",
    "        if bc >= confidence_lower_bound:\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(ft_mtx[:,s])\n",
    "            bc = cf_mtx[cid,s]\n",
    "            best_cfs.append(bc)\n",
    "            best_fts.append(ft_mtx[cid,s])\n",
    "            best_ass.append(as_mtx[cid,s])\n",
    "\n",
    "\n",
    "        cid = np.argmax(DT_ft_mtx_cp[:,s])\n",
    "        bc = DT_cf_mtx[cid,s]\n",
    "        print(\"DT\",cid,bc)\n",
    "        if bc >= confidence_lower_bound:\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "        else:\n",
    "            cid = np.argmax(DT_ft_mtx[:,s])\n",
    "            bc = DT_cf_mtx[cid,s]\n",
    "            DT_best_cfs.append(bc)\n",
    "            DT_best_fts.append(DT_ft_mtx[cid,s])\n",
    "            DT_best_ass.append(DT_as_mtx[cid,s])\n",
    "\n",
    "\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure(figsize=(5,4))\n",
    "    color1 = '#377EB8'  # Blue\n",
    "    color2 = '#E41A1C'  # Red\n",
    "    color3 = '#4DAF4A'  # green\n",
    "    plt.plot(support_range,best_cfs,'-o',color=color1,markersize=4)\n",
    "    plt.plot(support_range,best_fts,'--o',color=color1,markersize=4)\n",
    "    plt.plot(support_range,DT_best_cfs,'-o',color=color2,markersize=4)\n",
    "    plt.plot(support_range,DT_best_fts,'--o',color=color2,markersize=4)\n",
    "    plt.xlim(100,500)\n",
    "    plt.ylim(-0.2,1.)\n",
    "    # Creating custom lines for the color legend\n",
    "    custom_lines_color = [Line2D([0], [0], color=color1, lw=4),\n",
    "                          Line2D([0], [0], color=color2, lw=4)]\n",
    "    # Creating custom lines for the line style legend\n",
    "    custom_lines_style = [Line2D([0], [0], color='grey', lw=2, linestyle='-'),\n",
    "                          Line2D([0], [0], color='grey', lw=2, linestyle='--')]\n",
    "    plt.xticks(np.arange(50, 550, 50))\n",
    "    plt.xlabel('Specified minimum support')\n",
    "    plt.savefig('./compare_DT_AhR.svg')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
